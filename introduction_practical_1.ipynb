{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "introduction_practical_1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kimambo/Dissertate/blob/master/introduction_practical_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "IUOEMEVTbqb8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The mathematical challenge for the artificial neural network is to best optimize thousands or millions or whatever number of weights you have, so that your output layer results in what you were hoping for. Solving for this problem, and building out the layers of our neural network model is exactly what TensorFlow is for. TensorFlow is used for all things \"operations on tensors.\" A tensor in this case is nothing fancy. It's a multi-dimensional array.\n",
        "\n",
        "To install TensorFlow, simply do a:\n",
        "\n",
        "\n",
        "\n",
        "###\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_9D9oZM8bqcA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### pip install --upgrade tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "W8JxqsTNbqcC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Following the release of deep learning libraries, higher-level API-like libraries came out, which sit on top of the deep learning libraries, like TensorFlow, which make building, testing, and tweaking models even more simple. One such library that has easily become the most popular is Keras.\n",
        "\n",
        "Keras has become so popular, that it is now a superset, included with TensorFlow releases now! If you're familiar with Keras previously, you can still use it, but now you can use tensorflow.keras to call it. By that same token, if you find example code that uses Keras, you can use with the TensorFlow version of Keras too. In fact, you can just do something like:"
      ]
    },
    {
      "metadata": {
        "id": "TRabpe_BbqcE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-v9PUWQbqcP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For this tutorial, we are going to be using TensorFlow version 1.10. You can figure out your version:"
      ]
    },
    {
      "metadata": {
        "id": "jPixPrq7bqcQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "70cfbc00-1460-41c9-a254-ec052a3b7238"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b2d_bECGbqcV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once we've got tensorflow imported, we can then begin to prepare our data, model it, and then train it. For the sake of simplicity, we'll be using the most common \"hello world\" example for deep learning, which is the mnist dataset. It's a dataset of hand-written digits, 0 through 9. It's 28x28 images of these hand-written digits. We will show an example of using outside data as well, but, for now, let's load in this data:"
      ]
    },
    {
      "metadata": {
        "id": "2c38U0NYbqcW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "88c492f4-e6b7-445a-e3f7-62babae73375"
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Ky3brmebqcY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What exactly do we have here? Let's take a quick look.\n",
        "\n",
        "\n",
        "So the x_train data is the \"features.\" In this case, the features are pixel values of the 28x28 images of these digits 0-9. The y_train is the label (is it a 0,1,2,3,4,5,6,7,8 or a 9?)\n",
        "\n",
        "The testing variants of these variables is the \"out of sample\" examples that we will use. These are examples from our data that we're going to set aside, reserving them for testing the model.\n",
        "\n",
        "Neural networks are exceptionally good at fitting to data, so much so that they will commonly over-fit the data. Our real hope is that the neural network doesn't just memorize our data and that it instead \"generalizes\" and learns the actual problem and patterns associated with it.\n",
        "\n",
        "Let's look at this actual data:"
      ]
    },
    {
      "metadata": {
        "id": "IOdQl9FsbqcY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(x_train[15908])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SJX0zE7RhM08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "71fbfd48-e3e0-4e81-c579-1f4c0788be1c"
      },
      "cell_type": "code",
      "source": [
        "print(y_train[15908])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2J2NWunrg4Sh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be0e3724-5a0a-48ea-c6b6-5da0198a116d"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(x_train[..., np.newaxis].shape, y_train.shape, x_test.shape, y_test.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1) (60000,) (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0nkkXEm0bqcc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "d9b8c27f-2ada-4f56-dfb1-dcd89af22d57"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#normal charts inside notebooks\n",
        "%matplotlib inline \n",
        "\n",
        "plt.imshow(x_train[15908],cmap=plt.cm.binary)\n",
        "plt.show()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noAg/EBThB4Ii/EBQhB8I6v8AxclM/LnJmGQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "xe8oEsdMbqcf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "How about the value for y_train with the same index?\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "HFqQ_zzNbqcf",
        "colab_type": "code",
        "colab": {},
        "outputId": "1d6dc611-4cda-4251-e464-08ef46d45510"
      },
      "cell_type": "code",
      "source": [
        "print(y_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4DiVgUjlbqci",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It's generally a good idea to \"normalize\" your data. This typically involves scaling the data to be between 0 and 1, or maybe -1 and positive 1. In our case, each \"pixel\" is a feature, and each feature currently ranges from 0 to 255. Not quite 0 to 1. Let's change that with a handy utility function:"
      ]
    },
    {
      "metadata": {
        "id": "fgb_ZsUDbqcj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2444
        },
        "outputId": "a2debc1e-f2b1-42f0-a4df-150dec7355f0"
      },
      "cell_type": "code",
      "source": [
        "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
        "x_test = tf.keras.utils.normalize(x_test, axis=1)\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.08216044 0.2286589  0.3728098\n",
            "  0.30506548 0.08583808 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.08087653 0.38341541 0.36240278 0.37133624\n",
            "  0.48350001 0.4068725  0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.08861609 0.3824786  0.40758025 0.36240278 0.35218\n",
            "  0.44704564 0.43262392 0.06832372 0.00859123 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.01621743\n",
            "  0.095788   0.36759266 0.42460179 0.40758025 0.36240278 0.29765841\n",
            "  0.16116667 0.43262392 0.30326141 0.17468832 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.26434406\n",
            "  0.4023096  0.41354174 0.42460179 0.40758025 0.36240278 0.37133624\n",
            "  0.18419048 0.32446794 0.30326141 0.23912253 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.08411834 0.38597476\n",
            "  0.40390606 0.41518278 0.32013627 0.18365276 0.36384089 0.33597088\n",
            "  0.09017659 0.13562417 0.30565873 0.2405544  0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.07427511 0.39255225 0.40867916\n",
            "  0.4023096  0.29374592 0.02021913 0.12082418 0.17401086 0.03094469\n",
            "  0.         0.         0.30326141 0.34794476 0.12263192 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.04890249 0.2553207  0.41729294 0.37786605\n",
            "  0.33206506 0.13784725 0.         0.         0.         0.\n",
            "  0.         0.         0.30326141 0.36083161 0.40468535 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.00966301 0.22906954 0.38994434 0.39585101 0.11514373\n",
            "  0.03033287 0.04594908 0.         0.         0.         0.\n",
            "  0.         0.         0.30326141 0.36083161 0.47826451 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.07868449 0.32430069 0.38994434 0.10391089 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.30326141 0.36083161 0.47826451 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.27332506 0.3255876  0.29400565 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.30565873 0.36226348 0.48071715 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.33960736 0.33958568 0.32430069 0.17330859 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.30326141 0.36083161 0.3629905  0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.37982402 0.34786826 0.29598873 0.03868495 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.01343056 0.23176282 0.30326141 0.26632809 0.02943166 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.37982402 0.34786826 0.28698037 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.0103149\n",
            "  0.25134326 0.43262392 0.26969888 0.10166287 0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.37982402 0.34786826 0.18660159 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.0690291  0.24313682\n",
            "  0.48350001 0.29699976 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.38429254 0.34924869 0.28955419 0.         0.         0.\n",
            "  0.         0.         0.         0.18365276 0.34226929 0.3728098\n",
            "  0.31082143 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.37982402 0.34786826 0.32043997 0.22592013 0.0791702  0.04703054\n",
            "  0.13569967 0.29210488 0.37910874 0.40758025 0.3206977  0.24608394\n",
            "  0.10744445 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.37982402 0.34786826 0.32430069 0.38994434 0.37770784 0.34867468\n",
            "  0.4023096  0.41354174 0.42460179 0.31575387 0.18695382 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.1251185  0.27470549 0.32430069 0.38994434 0.41729294 0.40867916\n",
            "  0.4023096  0.38236201 0.24431452 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.03451074 0.16472416 0.38994434 0.41729294 0.40867916\n",
            "  0.2251018  0.06071843 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7GA4WhPbbqcl",
        "colab_type": "code",
        "colab": {},
        "outputId": "9dfce2ef-91c7-488a-c2ce-e9a25bc9a74b"
      },
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.00393124 0.02332955 0.02620568 0.02625207 0.17420356 0.17566281\n",
            "  0.28629534 0.05664824 0.51877786 0.71632322 0.77892406 0.89301644\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.05780486 0.06524513 0.16128198 0.22713296\n",
            "  0.22277047 0.32790981 0.36833534 0.3689874  0.34978968 0.32678448\n",
            "  0.368094   0.3747499  0.79066747 0.67980478 0.61494005 0.45002403\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.12250613 0.45858525 0.45852825 0.43408872 0.37314701\n",
            "  0.33153488 0.32790981 0.36833534 0.3689874  0.34978968 0.32420121\n",
            "  0.15214552 0.17865984 0.25626376 0.1573102  0.12298801 0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.04500225 0.4219755  0.45852825 0.43408872 0.37314701\n",
            "  0.33153488 0.32790981 0.28826244 0.26543758 0.34149427 0.31128482\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.1541463  0.28272888 0.18358693 0.37314701\n",
            "  0.33153488 0.26569767 0.01601458 0.         0.05945042 0.19891229\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.0253731  0.00171577 0.22713296\n",
            "  0.33153488 0.11664776 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.20500962\n",
            "  0.33153488 0.24625638 0.00291174 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.01622378\n",
            "  0.24897876 0.32790981 0.10191096 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.04586451 0.31235677 0.32757096 0.23335172 0.14931733 0.00129164\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.10498298 0.34940902 0.3689874  0.34978968 0.15370495\n",
            "  0.04089933 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.06551419 0.27127137 0.34978968 0.32678448\n",
            "  0.245396   0.05882702 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.02333517 0.12857881 0.32549285\n",
            "  0.41390126 0.40743158 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.32161793\n",
            "  0.41390126 0.54251585 0.20001074 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.06697006 0.18959827 0.25300993 0.32678448\n",
            "  0.41390126 0.45100715 0.00625034 0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.05110617 0.19182076 0.33339444 0.3689874  0.34978968 0.32678448\n",
            "  0.40899334 0.39653769 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.04117838 0.16813739\n",
            "  0.28960162 0.32790981 0.36833534 0.3689874  0.34978968 0.25961929\n",
            "  0.12760592 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.04431706 0.11961607 0.36545809 0.37314701\n",
            "  0.33153488 0.32790981 0.36833534 0.28877275 0.111988   0.00258328\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.05298497 0.42752138 0.4219755  0.45852825 0.43408872 0.37314701\n",
            "  0.33153488 0.25273681 0.11646967 0.01312603 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.37491383 0.56222061\n",
            "  0.66525569 0.63253163 0.48748768 0.45852825 0.43408872 0.359873\n",
            "  0.17428513 0.01425695 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.92705966 0.82698729\n",
            "  0.74473314 0.63253163 0.4084877  0.24466922 0.22648107 0.02359823\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CTd0s8Sfbqcn",
        "colab_type": "code",
        "colab": {},
        "outputId": "7e404a18-596a-4e8e-c5f2-38eb7281b478"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(x_train[0],cmap=plt.cm.binary)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd46330a050>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "CCxqIrtzbqcq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Now let's build our model!"
      ]
    },
    {
      "metadata": {
        "id": "QTsufby5bqcr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uzr5ptrObqct",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A sequential model is what you're going to use most of the time. It just means things are going to go in direct order. A feed forward model. No going backwards...for now.\n",
        "\n",
        "Now, we'll pop in layers. Recall our neural network image? Was the input layer flat, or was it multi-dimensional? It was flat. So, we need to take this 28x28 image, and make it a flat 1x784. There are many ways for us to do this, but keras has a Flatten layer built just for us, so we'll use that."
      ]
    },
    {
      "metadata": {
        "id": "MSP7rtL4bqcu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G7ja4MnZbqcw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This will serve as our input layer. It's going to take the data we throw at it, and just flatten it for us. Next, we want our hidden layers. We're going to go with the simplest neural network layer, which is just a Dense layer. This refers to the fact that it's a densely-connected layer, meaning it's \"fully connected,\" where each node connects to each prior and subsequent node. Just like our image."
      ]
    },
    {
      "metadata": {
        "id": "m56WkwYFbqcx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JgsvLx2bqcz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This layer has 128 units. The activation function is relu, short for rectified linear. Currently, relu is the activation function you should just default to. There are many more to test for sure, but, if you don't know what to use, use relu to start.\n",
        "\n",
        "Let's add another identical layer for good measure."
      ]
    },
    {
      "metadata": {
        "id": "ZEpHDGadbqc0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x1dOBEIxbqc3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we're ready for an output layer:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rJPReY0abqc4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mSnNRzAobqc_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is our final layer. It has 10 nodes. 1 node per possible number prediction. In this case, our activation function is a softmax function, since we're really actually looking for something more like a probability distribution of which of the possible prediction options this thing we're passing features through of is. Great, our model is done.\n",
        "\n",
        "Now we need to \"compile\" the model. This is where we pass the settings for actually optimizing/training the model we've defined."
      ]
    },
    {
      "metadata": {
        "id": "DX06G4NFbqdA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#loss function is normally used in information theory\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UiWYG6HAbqdD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Remember why we picked relu as an activation function? Same thing is true for the Adam optimizer. It's just a great default to start with.\n",
        "\n",
        "Next, we have our loss metric. Loss is a calculation of error. A neural network doesn't actually attempt to maximize accuracy. It attempts to minimize loss. Again, there are many choices, but some form of categorical crossentropy is a good start for a classification task like this.\n",
        "\n",
        "Now, we fit!"
      ]
    },
    {
      "metadata": {
        "id": "Yp12XUUfbqdE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "306053e8-e1e3-41b0-a6ed-4f6be45d4043"
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=4)\n"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "60000/60000 [==============================] - 9s 151us/sample - loss: 0.0547 - acc: 0.9825\n",
            "Epoch 2/4\n",
            "60000/60000 [==============================] - 10s 159us/sample - loss: 0.0416 - acc: 0.9860\n",
            "Epoch 3/4\n",
            "60000/60000 [==============================] - 9s 157us/sample - loss: 0.0336 - acc: 0.9886\n",
            "Epoch 4/4\n",
            "60000/60000 [==============================] - 10s 159us/sample - loss: 0.0251 - acc: 0.9915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2c484378d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "id": "o4e7wleYbqdG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we train, we can see loss goes down (yay), and accuracy improves quite quickly to 98-99% (double yay!)\n",
        "\n",
        "Now that's loss and accuracy for in-sample data. Getting a high accuracy and low loss might mean your model learned how to classify digits in general (it generalized)...or it simply memorized every single example you showed it (it overfit). This is why we need to test on out-of-sample data (data we didn't use to train the model).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "oHyNa7nYbqdG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "fb393c9a-5874-4c11-a781-9948fa52c8cc"
      },
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
        "print(val_loss)\n",
        "print(val_acc)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 45us/sample - loss: 0.0905 - acc: 0.9762\n",
            "0.09053532691081637\n",
            "0.9762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yPOlSPYKbqdL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It's going to be very likely your accuracy out of sample is a bit worse, same with loss. In fact, it should be a red flag if it's identical, or better.\n",
        "\n",
        "finally, make predictions!\n"
      ]
    },
    {
      "metadata": {
        "id": "0Rv6Zcy9bqdM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bd5ee420-6e52-4282-a954-372b89fe74bb"
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x_test)\n",
        "print(predictions.shape)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E3MU8KOlbqdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "bdda3fad-09db-42cc-fbee-1d50ccee08de"
      },
      "cell_type": "code",
      "source": [
        "print(predictions)\n",
        "#these are probability functions"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.08569895e-09 2.03500772e-08 6.42329951e-06 ... 9.99989629e-01\n",
            "  1.97378114e-09 7.01647629e-10]\n",
            " [3.27668759e-09 3.79268531e-05 9.99961257e-01 ... 2.82432827e-11\n",
            "  4.89636422e-11 1.04398386e-16]\n",
            " [1.34936409e-10 9.99982476e-01 2.49931099e-06 ... 5.02529565e-06\n",
            "  6.82655855e-06 1.16741763e-08]\n",
            " ...\n",
            " [9.79081074e-13 1.35701270e-10 4.96185974e-11 ... 1.46214436e-08\n",
            "  1.29369738e-07 5.17104763e-06]\n",
            " [2.27380458e-11 1.60424674e-10 9.78372851e-12 ... 2.56433341e-09\n",
            "  8.29611690e-06 6.01747723e-14]\n",
            " [7.58640650e-10 9.29079556e-12 7.35037736e-13 ... 1.38587731e-12\n",
            "  4.53904497e-10 1.93317519e-12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aCUw9JtnbqdS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That sure doesn't start off as helpful, but recall these are probability distributions. We can get the actual number pretty simply:\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "OpNNXNf8bqdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "e847cb15-6f95-44ed-feac-ce657a35e29a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(predictions[9999])\n",
        "print(y_test[9999])\n",
        "print(np.argmax(predictions[9999]))\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7.5864065e-10 9.2907956e-12 7.3503774e-13 2.3474207e-11 2.0941266e-09\n",
            " 4.1929416e-06 9.9999583e-01 1.3858773e-12 4.5390450e-10 1.9331752e-12]\n",
            "6\n",
            "6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2UD3NSwVbqdW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There's your prediction, let's look at the input:\n"
      ]
    },
    {
      "metadata": {
        "id": "Pk6gTATJbqdX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "7504006a-e3b2-411c-cfd1-b674eb7dbf16"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(x_test[9999],cmap=plt.cm.binary)\n",
        "plt.show()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWlqjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Tvua4kwnbqda",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Awesome! Okay, I think that covers all of the \"quick start\" types of things with Keras. This is just barely scratching the surface of what's available to you, so start poking around Tensorflow and Keras documentation."
      ]
    }
  ]
}